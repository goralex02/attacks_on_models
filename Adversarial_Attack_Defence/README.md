# Задание 1. Атаки adversarial на модель 
Цель: Демонстрация уязвимости модели к атаке.

1. Дана предобученная модель классификации изображений YOLOv8-cls.

2. Для данной модели проводим обучение на наборе данных SVHN. 

3. Используя библиотеку (например, art, cleverhans), создайте adversarial examples, которые изменяют предсказание модели и подготовьте inference для демонстрации.

# Задание 2: Атаки adversarial на модель, реализация защитных механизмов

(Выполняется после Задания №1)

 1. Модифицируйте модель или обучающий набор данных, чтобы она стала более устойчивой к атакам (например, с помощью adversarial training, нормализации входных данных или использования dropout).

 2. Проверьте защищенную модель на тех же adversarial examples.

 В папке data два файла натренированных в ходе выполнения задания моделей и файлик с названиями картинок для валидации.

Выполнение задания 2 в нижней трети ноутбука, до этого идёт задание 1.

Версия colab: https://colab.research.google.com/drive/1YxPaCB3B-v8-J4uuINC9SK9GpPkH6Svs 