# Задание 5. Защита от модели-имитатора (Model Stealing)

Цель: Проверка уязвимости модели классификации к краже.

* Задание:

 1. Выполните model stealing attack, создавая локальную модель, которая имитирует поведение удалённой модели (через API). Предлагается проверять уязвимость на модели классификации на табличных данных.

В тестовом задании под удаленной моделью можно взять обученную модель из открытых источников (Red Wine classification) и развернуть локально. Под API подразумевается функция predict выбранной модели.

Кроме вызова подобной функции, любое взаимодействие с моделью на первом этапе запрещено! 

def requestModel(X):

  result = model.predict(X)

  return result

 2. Определите меры защиты, чтобы снизить точность "украденной" модели. Это может включать в себя, например, дополнительную обработку датасетов для обучения модели, входных параметров API, выходных результатов функции.

Версия colab: https://colab.research.google.com/drive/1p3ff4hHEHzKHz70gBcuafzbMnvPt1hOd 
